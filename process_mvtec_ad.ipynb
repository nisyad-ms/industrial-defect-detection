{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./datasets/raw/mvtec-ad\"\n",
    "damage_category = \"transistor\"\n",
    "test_path = Path(dataset_path) / damage_category / \"test\"\n",
    "gt_mask_path = Path(dataset_path) / damage_category / \"ground_truth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_path.exists())\n",
    "print(gt_mask_path.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connected_components(gt_mask_path):\n",
    "    # mask = cv2.imread(gt_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask = Image.open(gt_mask_path)\n",
    "    mask_array = np.array(mask)\n",
    "\n",
    "    output = cv2.connectedComponentsWithStats(mask_array, 4, cv2.CV_32S)\n",
    "    numLabels, labels, stats, centroids = output\n",
    "    # exclude background and take only the bounding boxes\n",
    "    boxes = stats[1:, :4]\n",
    "    return boxes.tolist()  # x, y, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "tmp = Path(\"./000_mask.png\")\n",
    "tmp2 = get_connected_components(tmp)\n",
    "tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [entry.name for entry in test_path.iterdir() if entry.is_dir()]\n",
    "\n",
    "print(len(class_names))\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_to_id = {class_name: i+1 for i,\n",
    "                     class_name in enumerate(class_names)}\n",
    "class_names_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_type_to_annotations = {}\n",
    "file_name_to_boxes = {}\n",
    "\n",
    "for class_name in class_names:\n",
    "    if class_name == \"good\":\n",
    "        continue\n",
    "\n",
    "    class_path = gt_mask_path / class_name\n",
    "    gt_mask_paths = [(entry.name, entry) for entry in class_path.iterdir()]\n",
    "    for file_name, file_path in gt_mask_paths:\n",
    "        file_name_to_boxes[file_name] = get_connected_components(file_path)\n",
    "\n",
    "    damage_type_to_annotations[class_name] = file_name_to_boxes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_type_to_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import PIL.Image as Image\n",
    "\n",
    "images = []\n",
    "\n",
    "id = 0\n",
    "for root, dirs, files in os.walk(test_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".png\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            image = Image.open(file_path)\n",
    "            width, height = image.size\n",
    "\n",
    "            relative_file_path = os.path.relpath(file_path, test_path)\n",
    "\n",
    "            id += 1\n",
    "            single_image = {\n",
    "                \"id\": id,\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"file_name\": relative_file_path,\n",
    "                \"zip_file\": \"test_images.zip\"\n",
    "            }\n",
    "\n",
    "            images.append(single_image.copy())\n",
    "\n",
    "print(len(images))\n",
    "print(images[0])\n",
    "print(images[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "\n",
    "id = 0\n",
    "for image in images:\n",
    "    damage_type, img_name = image['file_name'].split(\"/\")\n",
    "\n",
    "    if damage_type == \"good\":\n",
    "        continue\n",
    "\n",
    "    image_name = img_name.split(\".\")[0]\n",
    "    mask_name = f\"{image_name}_mask.png\"\n",
    "\n",
    "    for bbox in damage_type_to_annotations[damage_type][mask_name]:\n",
    "        id += 1\n",
    "        single_annotation = {\n",
    "            \"id\": id,\n",
    "            \"category_id\": class_names_to_id[damage_type],\n",
    "            \"image_id\": image['id'],\n",
    "            \"bbox\": bbox\n",
    "        }\n",
    "\n",
    "        annotations.append(single_annotation.copy())\n",
    "\n",
    "print(len(annotations))\n",
    "print(annotations[0])\n",
    "print(annotations[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "\n",
    "for class_name, class_id in class_names_to_id.items():\n",
    "    category = {\n",
    "        \"id\": class_id,\n",
    "        \"name\": class_name}\n",
    "    categories.append(category.copy())\n",
    "\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "coco_json = {\n",
    "    \"images\": images,\n",
    "    \"annotations\": annotations,\n",
    "    \"categories\": categories\n",
    "}\n",
    "\n",
    "\n",
    "with open(f\"mvtec-ad_{damage_category}_test.json\", \"w\") as f:\n",
    "    json.dump(coco_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "\n",
    "img_path = \"datasets/raw/mvtec-ad/transistor/ground_truth/bent_lead/004_mask.png\"\n",
    "\n",
    "# thresh = Image.open(\"000_mask.png\")\n",
    "thresh = Image.open(img_path)\n",
    "thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = np.array(thresh)\n",
    "thresh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "output = cv2.connectedComponentsWithStats(\n",
    "    thresh, 4, cv2.CV_32S)\n",
    "(numLabels, labels, stats, centroids) = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(img_path)\n",
    "\n",
    "i = 1\n",
    "# Define the bounding box coordinates\n",
    "x, y, w, h = stats[i][0], stats[i][1], stats[i][2], stats[i][3]\n",
    "\n",
    "# Draw the bounding box on the image\n",
    "cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Convert the image from BGR to RGB\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image with bounding box\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_local_vision_dataset, TorchDataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "\n",
    "dataset = load_local_vision_dataset(dataset_name=\"mvtec_ad\",\n",
    "                                    dataset_config_path=\"./datasets.json\",\n",
    "                                    root_dir=\"./\",\n",
    "                                    task_type=\"object_detection\",\n",
    "                                    )\n",
    "\n",
    "\n",
    "transform = Compose([ToTensor()])\n",
    "\n",
    "dataset = TorchDataset(dataset, transform=None)\n",
    "sample_img, sample_tgt = dataset[70]\n",
    "print(sample_img)\n",
    "print(sample_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "data_loader_context = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "to_pil = ToPILImage()\n",
    "img = to_pil(sample_img.squeeze(0))\n",
    "\n",
    "bbox_ltrb = sample_tgt.squeeze(0).tolist()[1:]\n",
    "\n",
    "\n",
    "# Convert the bounding box coordinates from ltrb to xywh\n",
    "x, y, x2, y2 = bbox_ltrb\n",
    "w, h = x2 - x, y2 - y\n",
    "\n",
    "# Convert relative coordinates to absolute coordinates\n",
    "x, y, w, h = x * img.width, y * img.height, w * img.width, h * img.height\n",
    "\n",
    "\n",
    "# Create a figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(img)\n",
    "\n",
    "# Create a rectangle patch\n",
    "rect = patches.Rectangle((x, y), w, h, linewidth=2,\n",
    "                         edgecolor='r', facecolor='none')\n",
    "\n",
    "# Add the rectangle patch to the axes\n",
    "ax.add_patch(rect)\n",
    "\n",
    "# Show the image with the bounding box\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mimetypes import guess_type\n",
    "import io\n",
    "\n",
    "buffer = io.BytesIO()\n",
    "# You can change JPEG to PNG if you prefer\n",
    "img.save(buffer, format=\"JPEG\")\n",
    "\n",
    "guess_type(buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tgt.squeeze(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import conert_tensor_to_base64\n",
    "\n",
    "img_base64 = conert_tensor_to_base64(sample_img)\n",
    "img_base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_client import GPTClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gpt_client = GPTClient(api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "                       api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "                       api_version=\"2023-12-01-preview\",\n",
    "                       deployment_name=\"gpt4o-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gpt_client.get_response(\"\", \"describe the image\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import convert_to_base64\n",
    "\n",
    "base64_image, _ = convert_to_base64(\"./sample.png\")\n",
    "print(base64_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_client.get_response(base64_image, \"use the image to tell a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "request_body = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Describe this picture:\"\n",
    "                    },\n",
    "                {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": \"data:image/png;base64,\" + base64_image,\n",
    "                        }\n",
    "                    }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 200\n",
    "}\n",
    "\n",
    "print(request_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'api-key': os.getenv(\"AZURE_OPENAI_API_KEY\"), \n",
    "           'Content-Type': 'application/json'}\n",
    "endpoint = \"https://customvision-dev-aoai.openai.azure.com\"\n",
    "deployment_name = \"gpt4o-001\"\n",
    "\n",
    "url = f'{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-02-15-preview'\n",
    "\n",
    "\n",
    "response = requests.post(url, headers=headers, json=request_body, timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = response.json()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
