{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./datasets/raw/mvtec-ad\"\n",
    "damage_category = \"transistor\"\n",
    "test_path = Path(dataset_path) / damage_category / \"test\"\n",
    "gt_mask_path = Path(dataset_path) / damage_category / \"ground_truth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_path.exists())\n",
    "print(gt_mask_path.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connected_components(gt_mask_path):\n",
    "    # mask = cv2.imread(gt_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask = Image.open(gt_mask_path)\n",
    "    mask_array = np.array(mask)\n",
    "    \n",
    "    output = cv2.connectedComponentsWithStats(mask_array, 4 , cv2.CV_32S)\n",
    "    numLabels, labels, stats, centroids = output\n",
    "    boxes = stats[1:, :4] # exclude background and take only the bounding boxes\n",
    "    return boxes.tolist() # x, y, w, h\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "tmp = Path(\"./000_mask.png\")\n",
    "tmp2 = get_connected_components(tmp)\n",
    "tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [entry.name for entry in test_path.iterdir() if entry.is_dir()]\n",
    "\n",
    "print(len(class_names))\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_to_id = {class_name: i+1 for i, class_name in enumerate(class_names)}\n",
    "class_names_to_id   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_type_to_annotations = {}\n",
    "file_name_to_boxes = {}\n",
    "\n",
    "for class_name in class_names:\n",
    "    if class_name == \"good\":\n",
    "        continue\n",
    "    \n",
    "    class_path = gt_mask_path / class_name\n",
    "    gt_mask_paths = [(entry.name, entry) for entry in class_path.iterdir()]\n",
    "    for file_name, file_path in gt_mask_paths:\n",
    "        file_name_to_boxes[file_name] = get_connected_components(file_path)\n",
    "    \n",
    "    damage_type_to_annotations[class_name] = file_name_to_boxes.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_type_to_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import PIL.Image as Image\n",
    "\n",
    "images = []\n",
    "\n",
    "id = 0\n",
    "for root, dirs, files in os.walk(test_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".png\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            image = Image.open(file_path)\n",
    "            width, height = image.size\n",
    "            \n",
    "            relative_file_path = os.path.relpath(file_path, test_path)\n",
    "            \n",
    "            id += 1\n",
    "            single_image = {\n",
    "                \"id\": id,\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"file_name\": relative_file_path,\n",
    "                \"zip_file\": \"test_images.zip\"\n",
    "            }\n",
    "            \n",
    "            images.append(single_image.copy())\n",
    "\n",
    "print(len(images))\n",
    "print(images[0])\n",
    "print(images[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "\n",
    "id = 0\n",
    "for image in images:\n",
    "    damage_type, img_name = image['file_name'].split(\"/\")\n",
    "    \n",
    "    if damage_type == \"good\":\n",
    "        continue\n",
    "    \n",
    "    image_name = img_name.split(\".\")[0]\n",
    "    mask_name = f\"{image_name}_mask.png\"\n",
    "    \n",
    "    for bbox in damage_type_to_annotations[damage_type][mask_name]:\n",
    "        id += 1\n",
    "        single_annotation = {\n",
    "            \"id\": id,\n",
    "            \"category_id\": class_names_to_id[damage_type],\n",
    "            \"image_id\": image['id'],\n",
    "            \"bbox\": bbox\n",
    "        }\n",
    "        \n",
    "        annotations.append(single_annotation.copy())\n",
    "    \n",
    "print(len(annotations))\n",
    "print(annotations[0])\n",
    "print(annotations[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "\n",
    "for class_name, class_id in class_names_to_id.items():\n",
    "    category = {\n",
    "        \"id\": class_id,\n",
    "        \"name\": class_name}\n",
    "    categories.append(category.copy())\n",
    "\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_json = {\n",
    "    \"images\": images,\n",
    "    \"annotations\": annotations,\n",
    "    \"categories\": categories\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "with open(f\"mvtec-ad_{damage_category}_test.json\", \"w\") as f:\n",
    "    json.dump(coco_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "\n",
    "img_path = \"datasets/raw/mvtec-ad/transistor/ground_truth/bent_lead/004_mask.png\"\n",
    "\n",
    "# thresh = Image.open(\"000_mask.png\")\n",
    "thresh = Image.open(img_path)\n",
    "thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = np.array(thresh)\n",
    "thresh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "output = cv2.connectedComponentsWithStats(\n",
    "\tthresh, 4 , cv2.CV_32S)\n",
    "(numLabels, labels, stats, centroids) = output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(img_path)\n",
    "\n",
    "i = 1\n",
    "# Define the bounding box coordinates\n",
    "x, y, w, h = stats[i][0], stats[i][1], stats[i][2], stats[i][3]\n",
    "\n",
    "# Draw the bounding box on the image\n",
    "cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Convert the image from BGR to RGB\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image with bounding box\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbbcb0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbbc80>, <vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbb9e0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbb4d0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbbe30>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cb46a50>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cb47590>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbbec0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cb46a50>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbbbf0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbbe60>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbbbf0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbbd70>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbbc80>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbba2d0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbb4d0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbbcb0>, <vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbbe60>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec110>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbbbcb0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbeccb0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cb09df0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec1a0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec230>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec1d0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec200>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec110>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec2c0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec080>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec320>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec3b0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec3e0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec110>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec4a0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec470>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec500>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec110>, <vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbece00>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec590>, <vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec4a0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbecdd0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbece00>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec5c0>]\n",
      "[<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest object at 0x7f1f6cbec590>]\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from vision_datasets.common import Usages, DatasetHub\n",
    "\n",
    "dataset_infos_json_path = 'datasets.json'\n",
    "dataset_hub = DatasetHub(pathlib.Path(dataset_infos_json_path).read_text(), container_url=None, local_dir=\"./\")\n",
    "stanford_cars = dataset_hub.create_vision_dataset('mvtec_ad', version=1, usage=Usages.TEST)\n",
    "\n",
    "# note that you can pass multiple datasets.json to DatasetHub, it can combine them all\n",
    "# example: DatasetHub([ds_json1, ds_json2, ...])\n",
    "# note that you can specify multiple usages in create_manifest_dataset call\n",
    "# example dataset_hub.create_manifest_dataset('stanford-cars', version=1, usage=[Usages.TRAIN, Usages.VAL])\n",
    "\n",
    "for img, targets, sample_idx_str in stanford_cars:\n",
    "    # img.show()\n",
    "    # img.close()\n",
    "    print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'additional_info': {},\n",
       " '_label_data': [4, 0.310546875, 0.3037109375, 0.396484375, 0.41015625],\n",
       " 'label_path': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = {\n",
    "        \"name\": \"mvtec_ad\",\n",
    "        \"version\": 1,\n",
    "        \"description\": \"A sampled ms-coco dataset.\",\n",
    "        \"type\": \"object_detection\",\n",
    "        \"format\": \"coco\",\n",
    "        \"root_folder\": \"datasets/processed/mvtect_ad_transistor\",\n",
    "        \"test\": {\n",
    "            \"index_path\": \"test.json\",\n",
    "            \"files_for_local_usage\": [\n",
    "                \"test.zip\"\n",
    "            ]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_datasets.torch import TorchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "transform = transforms.Compose([ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TorchDataset(stanford_cars, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.4863, 0.4667, 0.4314,  ..., 0.5098, 0.5059, 0.5098],\n",
       "          [0.4784, 0.4824, 0.4745,  ..., 0.5490, 0.5333, 0.5176],\n",
       "          [0.4902, 0.4902, 0.4941,  ..., 0.5608, 0.5490, 0.5373],\n",
       "          ...,\n",
       "          [0.5294, 0.5412, 0.5451,  ..., 0.5569, 0.5608, 0.5725],\n",
       "          [0.5529, 0.5490, 0.5529,  ..., 0.5804, 0.5882, 0.5804],\n",
       "          [0.5176, 0.5216, 0.5216,  ..., 0.5569, 0.5529, 0.5569]],\n",
       " \n",
       "         [[0.3529, 0.3451, 0.3294,  ..., 0.4039, 0.3961, 0.3922],\n",
       "          [0.3569, 0.3490, 0.3490,  ..., 0.4039, 0.3961, 0.3961],\n",
       "          [0.3529, 0.3608, 0.3647,  ..., 0.4157, 0.4039, 0.4000],\n",
       "          ...,\n",
       "          [0.3804, 0.3961, 0.3922,  ..., 0.4118, 0.4196, 0.4235],\n",
       "          [0.3961, 0.4000, 0.4078,  ..., 0.4235, 0.4235, 0.4275],\n",
       "          [0.3765, 0.3804, 0.3725,  ..., 0.3882, 0.3961, 0.4000]],\n",
       " \n",
       "         [[0.3098, 0.3176, 0.2824,  ..., 0.3608, 0.3529, 0.3569],\n",
       "          [0.3098, 0.3176, 0.2902,  ..., 0.3529, 0.3490, 0.3529],\n",
       "          [0.3059, 0.3059, 0.3020,  ..., 0.3451, 0.3451, 0.3490],\n",
       "          ...,\n",
       "          [0.3490, 0.3373, 0.3333,  ..., 0.3490, 0.3725, 0.3608],\n",
       "          [0.3686, 0.3451, 0.3412,  ..., 0.3608, 0.3647, 0.3725],\n",
       "          [0.3412, 0.3176, 0.3294,  ..., 0.3451, 0.3373, 0.3451]]]),\n",
       " [<vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest at 0x7f1f7f53e720>],\n",
       " '70')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[70]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, targets, _ = dataset[0]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 5))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tmp = torch.tensor([t.label_data for t in targets], dtype=torch.float32).reshape(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('0',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('1',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('2',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('3',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('4',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('5',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('6',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('7',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('8',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('9',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('10',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('11',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('12',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('13',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('14',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('15',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('16',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('17',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('18',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('19',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('20',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('21',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('22',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('23',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('24',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('25',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('26',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('27',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('28',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('29',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('30',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('31',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('32',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('33',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('34',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('35',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('36',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('37',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('38',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('39',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('40',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('41',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('42',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('43',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('44',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('45',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('46',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('47',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('48',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('49',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('50',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('51',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('52',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('53',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('54',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('55',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('56',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('57',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('58',)\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "[]\n",
      "('59',)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 182, in collate\n    clone[i] = collate(samples, collate_fn_map=collate_fn_map)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 191, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest'>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 316, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 189, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 191, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_idx_str\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 182, in collate\n    clone[i] = collate(samples, collate_fn_map=collate_fn_map)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 191, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest'>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 316, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 189, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nisyad/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 191, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'vision_datasets.image_object_detection.manifest.ImageObjectDetectionLabelManifest'>\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    imgs, targets, sample_idx_str = batch\n",
    "    print(imgs.shape)\n",
    "    print(targets)\n",
    "    print(sample_idx_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
